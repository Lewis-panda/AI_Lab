我在做實驗(機器學習相關)，以下是實驗步驟，然後我會給你不完整的code 請你幫我把code 完成 
實驗步驟
1. 讀取資料，其中包含訓練資料與測試資料，請將資料路徑設為您所使用的
電腦中的路徑
訓練資料: 資料包含 4 位受測者在 7 個位置時收集之 CSI 資料，每種資
料個 600 筆，總計 14400 筆資料
測試資料: 同上，但於不同時間收集
2. 資料前處理(Reshape, One-hot Encoder)
3. 建立包含 Input Layer、4 層 1D-CNN 網路(filter: 32, 64, 128, 128)並選用
tf.nn.leaky_relu 作為激活函數
4. 在利用 1D-CNN 提取特徵後，利用 tf.keras.layers.Flatten()將 tensor
的維度轉換成一維
5. 建立兩層 DNN 網路(Node: 256, 64)與 Output Layer(Node: 10)進行特徵映射
(Feature Mapping)
6. 編譯(compile)網路架構，loss 選擇"categorical_crossentropy"，optimizer 選
擇"Adam"，並利用 model.summary()在 console 裡印出如 Lab1 的圖
7. 進行神經網路模型的訓練 model.fit()
8. 畫出訓練過程中的精準度(Accuracy)與損失函數(Loss Function)


import random
import datetime
from functools import reduce
import scipy.io as sio
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.metrics import confusion_matrix
from sklearn.preprocessing import OneHotEncoder

def read_csi_samples(filename,sample_num):
    data = sio.loadmat(filename)
    result = []
    for d in data['original_csi']:
        result.append(d[0]['csi'][0][0])
        result = result[0:sample_num]
    return np.abs(np.array(result).reshape((-1, Nt*Nr, Ns)).transpose((0, 2, 1)))
    #return np.abs(np.array(result).reshape((-1, Nt*Nr*Ns)))

def get_cnn_block(filter):
    block = tf.keras.Sequential([
            tf.keras.layers.Conv1D(filter, 5, padding='same'),
            tf.keras.layers.MaxPool1D(padding='same'),
            tf.keras.layers.BatchNormalization(),
            tf.keras.layers.LeakyReLU(),
            tf.keras.layers.SpatialDropout1D(0.1),      
    ])
    return block


# ROOT_DIR = 'C:/Users/Mint/Desktop/AICourse'
ROOT_DIR = 'drive/MyDrive/nctu'
Nap, Nt, Nr, Ns, M = 2, 2, 2, 56, 6
dataset_num = 6*4

train_database = [f'{ROOT_DIR}/data/{p}(mat)/{p}{i}_1' for i in range(1, 7) for p in ['db', 'a', 'c', 'j']]
test_database = [f'{ROOT_DIR}/data/{p}(mat)/{p}{i}_2' for i in range(1, 7) for p in ['db', 'a', 'c', 'j']]
sample_num = 600 #@param
train_dataset = np.array([read_csi_samples(ds,sample_num) for ds in train_database])
test_dataset = np.array([read_csi_samples(ds,sample_num) for ds in test_database])


train_dataset = np.reshape(train_dataset,[dataset_num*sample_num,Ns,Nt*Nr])
test_dataset = np.reshape(test_dataset,[dataset_num*sample_num,Ns,Nt*Nr])

labels = np.zeros(dataset_num*sample_num)

for i in range(dataset_num):
    for j in range(sample_num):
        temp = i % 6
        labels[(i-1)*sample_num+j] =  i % 6
labels = np.reshape(labels,(dataset_num*sample_num,1))
onehotencoder = OneHotEncoder()
training_label = onehotencoder.fit_transform(labels).toarray()
test_label = training_label
inputs = keras.Input(shape = (56,4))

#your code
model.evaluate(test_dataset, test_label, batch_size = 256) 
y_pred = onehotencoder.inverse_transform(model.predict(test_dataset))  
y_true = onehotencoder.inverse_transform(test_label)    
print('\nConfusion Matrix:')
print(confusion_matrix(y_true,y_pred))

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()
# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()